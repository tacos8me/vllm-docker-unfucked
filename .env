# =============================================================================
# vLLM Blackwell Docker Configuration
# =============================================================================

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================
# Path to your models directory on the host
MODELS_PATH=/models

# Model name to serve (relative to MODELS_PATH or HuggingFace model ID)
MODEL_NAME=microsoft/DialoGPT-medium
# Examples:
# MODEL_NAME=meta-llama/Llama-2-7b-chat-hf
# MODEL_NAME=microsoft/DialoGPT-medium
# MODEL_NAME=/workspace/models/your-local-model

# =============================================================================
# PERFORMANCE CONFIGURATION
# =============================================================================
# Number of GPUs to use for tensor parallelism
TENSOR_PARALLEL_SIZE=1

# Maximum model sequence length
MAX_MODEL_LEN=2048

# Maximum number of sequences to process in parallel
MAX_NUM_SEQS=2048

# GPU memory utilization (0.0 to 1.0)
GPU_MEMORY_UTIL=0.90

# Data type for model weights (auto, float16, bfloat16)
DTYPE=auto

# =============================================================================
# SYSTEM CONFIGURATION
# =============================================================================
# Port to expose the API server
VLLM_PORT=8080

# Shared memory size for multi-GPU communication
SHM_SIZE=16g

# Memory limit for container
MEM_LIMIT=64g

# CUDA devices to use (all, or specific like "0,1")
CUDA_VISIBLE_DEVICES=all

# =============================================================================
# PATHS CONFIGURATION
# =============================================================================
# Cache directory for HuggingFace models
CACHE_PATH=./cache

# Dedicated Hugging Face cache path (used inside container at /root/.cache/huggingface)
HF_CACHE_PATH=./hf_cache

# Logs directory
LOGS_PATH=./logs

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
# Logging level (DEBUG, INFO, WARNING, ERROR)
VLLM_LOGGING_LEVEL=INFO

# =============================================================================
# ADVANCED CONFIGURATION
# =============================================================================
# Additional arguments to pass to vLLM server
ADDITIONAL_ARGS=

# Optional build-time version pins (leave empty to use latest compatible)
# TORCH_VERSION=
# TORCHVISION_VERSION=
# TORCHAUDIO_VERSION=
TORCH_CUDA_INDEX_URL=https://download.pytorch.org/whl/cu129
# VLLM_REF=

# Examples of additional arguments:
# ADDITIONAL_ARGS=--enable-lora --max-lora-rank 16
# ADDITIONAL_ARGS=--quantization awq
# ADDITIONAL_ARGS=--enable-prefix-caching

# =============================================================================
# OPTIONAL: MONITORING CONFIGURATION
# =============================================================================
# Grafana admin password (if using monitoring)
GRAFANA_PASSWORD=admin123

# Prometheus metrics port exposed on host
METRICS_PORT=8081

# Served model name (visible to OpenAI-compatible clients)
SERVED_MODEL_NAME=

# Performance/threading controls
OMP_NUM_THREADS=8
MKL_NUM_THREADS=8

# NCCL runtime tuning (set for single-node without Infiniband)
NCCL_DEBUG=WARN
NCCL_IB_DISABLE=1
