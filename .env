# =============================================================================
# vLLM Blackwell Docker Configuration
# =============================================================================

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================
# Path to your models directory on the host
MODELS_PATH=/models

# Model name to serve (relative to MODELS_PATH or HuggingFace model ID)
MODEL_NAME=/workspace/models/glm-air

# Served model name (VLLM) (visible to OpenAI-compatible clients)
SERVED_MODEL_NAME=glm/air
# Examples:
# MODEL_NAME=meta-llama/Llama-2-7b-chat-hf
# MODEL_NAME=microsoft/DialoGPT-medium
# MODEL_NAME=/workspace/models/your-local-model

# =============================================================================
# PERFORMANCE CONFIGURATION (VLLM)
# =============================================================================
# Number of GPUs to use for tensor parallelism
TENSOR_PARALLEL_SIZE=2

# Maximum model sequence length
MAX_MODEL_LEN=120000

# Maximum number of sequences to process in parallel
MAX_NUM_SEQS=4096

# GPU memory utilization (0.0 to 1.0)
GPU_MEMORY_UTIL=0.95

# Data type for model weights (auto, float16, bfloat16)
DTYPE=float16

# KV cache data type (auto, fp8, fp8_e5m2, fp8_e4m3)
KV_CACHE_DTYPE=fp8

# Quantization method (none, awq, gptq, squeezellm, marlin, compressed-tensors)
QUANTIZATION=marlin

# Enable prefix caching for better performance with repeated prompts
ENABLE_PREFIX_CACHING=true

# Block size for PagedAttention (16, 32)
BLOCK_SIZE=16

# Swap space in GiB (for CPU offloading when GPU memory is full)
SWAP_SPACE=4

# Maximum number of parallel loading workers
MAX_PARALLEL_LOADING_WORKERS=4

# =============================================================================
# SYSTEM CONFIGURATION
# =============================================================================
# Port to expose the API server (VLLM)
VLLM_PORT=8080

# Shared memory size for multi-GPU communication (VLLM)
SHM_SIZE=32g

# Memory limit for container (Docker)
MEM_LIMIT=256g

# CUDA devices to use (all, or specific like "0,1") (Docker)
CUDA_VISIBLE_DEVICES=0,1

# =============================================================================
# PATHS CONFIGURATION
# =============================================================================
# Cache directory for HuggingFace models
CACHE_PATH=./cache

# Dedicated Hugging Face cache path (used inside container at /root/.cache/huggingface)
HF_CACHE_PATH=./hf_cache

# Logs directory
LOGS_PATH=./logs

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
# Logging level (DEBUG, INFO, WARNING, ERROR)
VLLM_LOGGING_LEVEL=INFO

# =============================================================================
# ADVANCED CONFIGURATION
# =============================================================================
# Additional arguments to pass to vLLM server
ADDITIONAL_ARGS=--enable-chunked-prefill --max-num-batched-tokens 4096 --enable-prefix-caching --enforce-eager --enable-auto-tool-choice --reasoning-parser glm45 --tool-call-parser glm45 --load-format auto --disable-log-stats

# Optional build-time version pins (leave empty to use latest compatible)
# TORCH_VERSION=
# TORCHVISION_VERSION=
# TORCHAUDIO_VERSION=
TORCH_CUDA_INDEX_URL=https://download.pytorch.org/whl/cu129
# VLLM_REF=

# Build optimization settings
MAX_JOBS=32
NVCC_THREADS=16

# Examples of additional arguments:
# ADDITIONAL_ARGS=--enable-lora --max-lora-rank 16
# ADDITIONAL_ARGS=--quantization awq
# ADDITIONAL_ARGS=--enable-prefix-caching

# =============================================================================
# MONITORING CONFIGURATION (ENABLED)
# =============================================================================
# Enable monitoring stack
ENABLE_MONITORING=true

# Grafana admin password
GRAFANA_PASSWORD=admin123

# Prometheus metrics port exposed on host
METRICS_PORT=8081

# Grafana web interface port
GRAFANA_PORT=3000

# Prometheus web interface port
PROMETHEUS_PORT=9090

# Performance/threading controls
OMP_NUM_THREADS=8
MKL_NUM_THREADS=8

# NCCL runtime tuning (set for single-node without Infiniband)
NCCL_DEBUG=WARN
NCCL_IB_DISABLE=1

# ====================================================================================== #
# ADVANCED PERFORMANCE TUNING -- https://docs.vllm.ai/en/v0.7.3/serving/engine_args.html #
# ====================================================================================== #
# Enable CUDA graphs for better performance
VLLM_USE_MODELSCOPE=false
VLLM_WORKER_MULTIPROC_METHOD=spawn
VLLM_ENGINE_ITERATION_TIMEOUT_S=60
VLLM_API_KEY=

# Ray configuration for distributed serving
RAY_DISABLE_IMPORT_WARNING=1

# HuggingFace optimizations
HF_HUB_ENABLE_HF_TRANSFER=1
HF_HUB_DISABLE_TELEMETRY=1

# Tokenizer settings
TOKENIZER_MODE=auto
#TRUST_REMOTE_CODE=true

# Chat template content format (string, openai, none)
CHAT_TEMPLATE_CONTENT_FORMAT=openai

# Tool calling configuration
ENABLE_AUTO_TOOL_CHOICE=true
TOOL_CALL_PARSER=glm45
CUSTOM_CHAT_TEMPLATE=

# Combined tool calling arguments (automatically generated based on above settings)
# Enable this line for tool calling support:
TOOL_CALLING_ARGS=
# Disable tool calling by commenting out or setting to empty:
# TOOL_CALLING_ARGS=

# API server settings
API_KEY=
DISABLE_LOG_STATS=false
DISABLE_LOG_REQUESTS=false
