services:
  vllm-server:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        TORCH_VERSION: ${TORCH_VERSION}
        TORCHVISION_VERSION: ${TORCHVISION_VERSION}
        TORCHAUDIO_VERSION: ${TORCHAUDIO_VERSION}
        TORCH_CUDA_INDEX_URL: ${TORCH_CUDA_INDEX_URL}
        VLLM_REF: ${VLLM_REF}
        MAX_JOBS: ${MAX_JOBS}
        NVCC_THREADS: ${NVCC_THREADS}
    image: vllm-blackwell:latest
    container_name: vllm-blackwell-server
    restart: unless-stopped
    
    # GPU access for Windows Docker Desktop compatibility
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    # Port mapping
    ports:
      - "${VLLM_PORT:-8080}:8080"
    
    # Volume mounts
    volumes:
      - "${MODELS_PATH:-./models}:/workspace/models:ro"
      - "${CACHE_PATH:-./cache}:/root/.cache:rw"
      - "${LOGS_PATH:-./logs}:/opt/vllm/logs:rw"
      - "${HF_CACHE_PATH:-./hf_cache}:/root/.cache/huggingface:rw"
    
    # Environment variables
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-all}
      - NVIDIA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-all}
      - NVIDIA_DRIVER_CAPABILITIES=all
      - VLLM_FLASH_ATTN_VERSION=2
      - TORCH_CUDA_ARCH_LIST=9.0;10.0;12.0+PTX
      - PYTHONUNBUFFERED=1
      - VLLM_LOGGING_LEVEL=${VLLM_LOGGING_LEVEL:-INFO}
      - HF_HOME=/root/.cache/huggingface
      - HF_HUB_ENABLE_HF_TRANSFER=1
      - HF_TOKEN=${HF_TOKEN:-}
      - OMP_NUM_THREADS=${OMP_NUM_THREADS:-8}
      - MKL_NUM_THREADS=${MKL_NUM_THREADS:-8}
      - CUDA_DEVICE_MAX_CONNECTIONS=1
      - NCCL_DEBUG=${NCCL_DEBUG:-WARN}
      - NCCL_IB_DISABLE=${NCCL_IB_DISABLE:-1}
    
    # Shared memory for multi-GPU setups
    shm_size: ${SHM_SIZE:-16g}
    
    
    # Resource limits (optional)
    mem_limit: ${MEM_LIMIT:-64g}
    
    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    
    # Default command - serve a model
    command: >
      python -m vllm.entrypoints.openai.api_server
      --model ${MODEL_NAME:-/workspace/models}
      --served-model-name ${SERVED_MODEL_NAME:-${MODEL_NAME:-/workspace/models}}
      --host 0.0.0.0
      --port 8080
      --tensor-parallel-size ${TENSOR_PARALLEL_SIZE:-1}
      --max-model-len ${MAX_MODEL_LEN:-4096}
      --max-num-seqs ${MAX_NUM_SEQS:-256}
      --gpu-memory-utilization ${GPU_MEMORY_UTIL:-0.95}
      --dtype ${DTYPE:-auto}
      --trust-remote-code
      ${ADDITIONAL_ARGS:-}
    
    networks:
      - vllm-network

  # Monitoring with Prometheus
  prometheus:
    image: prom/prometheus:latest
    container_name: vllm-prometheus
    restart: unless-stopped
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--storage.tsdb.retention.time=30d'
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9090/-/healthy || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - vllm-network

  # Grafana dashboard
  grafana:
    image: grafana/grafana:latest
    container_name: vllm-grafana
    restart: unless-stopped
    ports:
      - "${GRAFANA_PORT:-3000}:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin123}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS=grafana-piechart-panel,grafana-lokiexplore-app,grafana-exploretraces-app,grafana-pyroscope-app,grafana-metricsdrilldown-app
      - GF_LOG_LEVEL=warn
      - GF_INSTALL_PLUGINS=
      - GF_PLUGIN_ADMIN_ENABLED=false
    volumes:
      - grafana-storage:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./grafana/datasources:/etc/grafana/provisioning/datasources:ro
    depends_on:
      - prometheus
    command: >
      sh -c "
        echo 'Waiting for Prometheus to be ready...' &&
        until wget --spider --quiet http://prometheus:9090/-/ready 2>/dev/null; do
          echo 'Prometheus not ready, waiting 5 seconds...'
          sleep 5
        done &&
        echo 'Prometheus is ready! Starting Grafana...' &&
        /run.sh
      "
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - vllm-network

# Named volumes for persistence
volumes:
  grafana-storage:
    driver: local
  prometheus-data:
    driver: local

# Optional: Custom network
networks:
  vllm-network:
    driver: bridge