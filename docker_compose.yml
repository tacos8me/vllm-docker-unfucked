version: '3.8'

services:
  vllm-server:
    build:
      context: .
      dockerfile: Dockerfile
    image: vllm-blackwell:latest
    container_name: vllm-blackwell-server
    restart: unless-stopped
    
    # GPU access for Blackwell
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    # Port mapping
    ports:
      - "${VLLM_PORT:-8080}:8080"
    
    # Volume mounts
    volumes:
      - "${MODELS_PATH:-./models}:/workspace/models:ro"
      - "${CACHE_PATH:-./cache}:/root/.cache:rw"
      - "${LOGS_PATH:-./logs}:/opt/vllm/logs:rw"
    
    # Environment variables
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-all}
      - VLLM_FLASH_ATTN_VERSION=2
      - TORCH_CUDA_ARCH_LIST=9.0;10.0;12.0+PTX
      - PYTHONUNBUFFERED=1
      - VLLM_LOGGING_LEVEL=${VLLM_LOGGING_LEVEL:-INFO}
    
    # Shared memory for multi-GPU setups
    shm_size: ${SHM_SIZE:-16g}
    
    # IPC mode for GPU communication
    ipc: host
    
    # Resource limits (optional)
    mem_limit: ${MEM_LIMIT:-64g}
    
    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    
    # Default command - serve a model
    command: >
      python -m vllm.entrypoints.openai.api_server
      --model ${MODEL_NAME:-/workspace/models}
      --host 0.0.0.0
      --port 8080
      --tensor-parallel-size ${TENSOR_PARALLEL_SIZE:-1}
      --max-model-len ${MAX_MODEL_LEN:-4096}
      --max-num-seqs ${MAX_NUM_SEQS:-256}
      --gpu-memory-utilization ${GPU_MEMORY_UTIL:-0.95}
      --dtype ${DTYPE:-auto}
      --trust-remote-code
      ${ADDITIONAL_ARGS:-}

  # Optional: Monitoring with Prometheus (uncomment if needed)
  # prometheus:
  #   image: prom/prometheus:latest
  #   container_name: vllm-prometheus
  #   ports:
  #     - "9090:9090"
  #   volumes:
  #     - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
  #   command:
  #     - '--config.file=/etc/prometheus/prometheus.yml'
  #     - '--storage.tsdb.path=/prometheus'
  #     - '--web.console.libraries=/etc/prometheus/console_libraries'
  #     - '--web.console.templates=/etc/prometheus/consoles'

  # Optional: Grafana dashboard (uncomment if needed)
  # grafana:
  #   image: grafana/grafana:latest
  #   container_name: vllm-grafana
  #   ports:
  #     - "3000:3000"
  #   environment:
  #     - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
  #   volumes:
  #     - grafana-storage:/var/lib/grafana

# Optional: Named volumes for persistence
volumes:
  grafana-storage:
    driver: local

# Optional: Custom network
networks:
  vllm-network:
    driver: bridge